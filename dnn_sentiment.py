# -*- coding: utf-8 -*-
"""DNN_sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1317ABsJ4fcjwUF2UEnYo10CUDw4cfupZ
"""
#
# pip install -U accelerate
# pip install -U transformers
# pip install -U deep-translator
# pip install -U datasets
# pip install spacy
#
# python -m spacy download ro_core_news_sm
#
# pip install stanza
#
# pip install -U keras_preprocessing
# pip install --upgrade tensorflow

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import json
import re
import os
import shutil
import datasets
import spacy
import stanza
import nltk
import keras
import logging

from multiprocessing import Pool
from sklearn.neighbors import KNeighborsClassifier
from gensim.models.doc2vec import TaggedDocument
from gensim.models import Doc2Vec
from sklearn.decomposition import TruncatedSVD
from sklearn.decomposition import NMF
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from keras.models import Sequential
from numpy import array
from numpy import asarray
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, classification_report
from collections import Counter
from spacy.lang.ro import Romanian
from transformers import BertTokenizer, TFBertForSequenceClassification
from transformers import InputExample, InputFeatures
from deep_translator import GoogleTranslator
# from google.colab import files
from datasets import Dataset
# from google.colab import files
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from tensorflow.keras.layers import GlobalMaxPooling1D
from keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import regularizers
from sklearn.model_selection import train_test_split

from gensim.models import Word2Vec

# from keras.preprocessing.text import Tokenizer
from keras.layers import TextVectorization
from keras.utils import text_dataset_from_directory
from keras_preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Embedding
from keras.layers import Conv1D
from keras.layers import MaxPooling1D

from keras.utils import text_dataset_from_directory

nltk.download('stopwords')
nltk.download('punkt')

romanian_alphabet_regex = re.compile(r'[A-Za-zĂăÂâÎîȘșȚț]')


stanza.download('ro')
nlp = stanza.Pipeline('ro')

stop_words = set(stopwords.words('romanian'))
print(stop_words)


def read_vocab(file_path):
  read_vocab = set()

  with open(file_path, 'r') as file:
    for line_number, line in enumerate(file, start=1):
      # Split each line by spaces
      parts = line.strip().split()

      # Check if there are exactly two parts (word and count)
      if len(parts) == 2:
        # Create a tuple (word, count) and add it to the set
        word, count = parts[0], int(parts[1])

        # Create a tuple (word, count) and add it to the set
        read_vocab.add((word, count))
      else:
        # Print lines with unexpected structure
        print(f"Error in line {line_number}: {line}")
  return(Counter(dict(read_vocab)))

def read_dset(file_path):
  with open(file_path, 'r') as file:
    data = json.load(file)
  return data


def get_vocabs():
  # Define the file path
  file_path = 'vocab_pos.txt'
  vocab_pos=read_vocab(file_path)

  file_path = 'vocab_neg.txt'
  vocab_neg=read_vocab(file_path)

  file_path = 'vocab_pos_nlemm.txt'
  vocab_pos_nlemm=read_vocab(file_path)

  file_path = 'vocab_neg_nlemm.txt'
  vocab_neg_nlemm=read_vocab(file_path)

  file_path = 'vocab_clean.txt'
  clean_voc=read_vocab(file_path)

  # print(len(vocab_pos))
  # print(vocab_pos)
  #
  # print(len(vocab_neg))
  # print(vocab_neg)
  #
  # print(len(vocab_pos_nlemm))
  # print(vocab_pos_nlemm)
  #
  # print(len(vocab_neg_nlemm))
  # print(vocab_neg_nlemm)
  #
  # print(len(clean_voc))
  # print(clean_voc)

  return vocab_pos, vocab_neg, vocab_pos_nlemm, vocab_neg_nlemm, clean_voc


def filter_alphabetic_tokens(tokens):
    # Define the Romanian alphabet range
    romanian_alphabet = "aăâbcddeefghiîjklmnopqrsștțuvwxyźâăîşţ"

    # Build the regex pattern for filtering
    pattern = re.compile(f"[^{romanian_alphabet}]")

    # Filter out tokens that contain non-alphabetic characters
    filtered_tokens = [token for token in tokens if not pattern.search(token)]

    return filtered_tokens

def clean_line_neg(line):
  rev=line['title']+' . '+ line['content']
  # rev=i['content']
  # rev = re.sub('(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', r' ', rev)
  rev = rev.lower()
  # rev = re.sub(r'\s+', ' ', rev)
  # rev = re.sub(r'(?<! )(?=[.,!?()"\'/:;/\\\[^\]])|(?<=[.,!?()"\'/:;/\\\[^\]])(?! )', r' ', rev)
  rev = re.sub(r'[.,!?()"\'/:;/\\\-]', ' ', rev)
  rev = re.sub(r'\s+', ' ', rev)
  # stop_words = set(stopwords.words('romanian'))
  words=rev.split()
  # words=[word for word in words if word.lower() not if not any(char.isdigit() for char in word)]
  words = filter_alphabetic_tokens(words)
  vocab_neg_nlemm.update(words)
  # words = [word for word in words if word.lower() not in stop_words]
  # rev = ' '.join(filtered_words)
  # rev = ' '.join([word for word in rev.split() if word not in stop_words])
  rev= ' '.join(words)
  doc = nlp(rev)
  lemmatized_tokens = [word.lemma for sent in doc.sentences for word in sent.words]
  # print(lemmatized_tokens)
  lemmatized_tokens=[token for token in lemmatized_tokens if isinstance(token, str)]
  lemmatized_tokens=[token for token in lemmatized_tokens if not token.isnumeric()]
  lemmatized_tokens=[token for token in lemmatized_tokens if not any(char.isdigit() for char in token)]
  vocab_neg.update(lemmatized_tokens)
  # print (token for token in lemmatized_tokens if not any(char.isdigit() for char in token))
  # for token in lemmatized_tokens:
  #   if any(char.isdigit() for char in token):
  #     print(token)
    # print(token)
    # print(token)
  rev=' '.join(lemmatized_tokens)
  return rev

def clean_line_pos(line):
  rev=line['title']+' . '+ line['content']
  # rev=i['content']
  # rev = re.sub('(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', r' ', rev)
  rev = rev.lower()
  # rev = re.sub(r'\s+', ' ', rev)
  # rev = re.sub(r'(?<! )(?=[.,!?()"\'/:;/\\\[^\]])|(?<=[.,!?()"\'/:;/\\\[^\]])(?! )', r' ', rev)
  rev = re.sub(r'[.,!?()"\'/:;/\\\-]', ' ', rev)
  rev = re.sub(r'\s+', ' ', rev)
  # stop_words = set(stopwords.words('romanian'))
  words=rev.split()
  # words=[word for word in words if word.lower() not if not any(char.isdigit() for char in word)]
  words = filter_alphabetic_tokens(words)
  vocab_pos_nlemm.update(words)
  rev= ' '.join(words)
  # words = [word for word in words if word.lower() not in stop_words]
  # rev = ' '.join(filtered_words)
  # rev = ' '.join([word for word in rev.split() if word not in stop_words])
  doc = nlp(rev)
  lemmatized_tokens = [word.lemma for sent in doc.sentences for word in sent.words]
  # print(lemmatized_tokens)
  lemmatized_tokens=[token for token in lemmatized_tokens if isinstance(token, str)]
  lemmatized_tokens=[token for token in lemmatized_tokens if not token.isnumeric()]
  lemmatized_tokens=[token for token in lemmatized_tokens if not any(char.isdigit() for char in token)]
  vocab_pos.update(lemmatized_tokens)
  # print (token for token in lemmatized_tokens if not any(char.isdigit() for char in token))
  # for token in lemmatized_tokens:
  #   if any(char.isdigit() for char in token):
  #     print(token)
    # print(token)
    # print(token)
  rev=' '.join(lemmatized_tokens)
  return rev


def clean_line(line):
  rev=line['title']+' . '+ line['content']
  # rev=i['content']
  # rev = re.sub('(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', r' ', rev)
  rev = rev.lower()
  # rev = re.sub(r'\s+', ' ', rev)
  # rev = re.sub(r'(?<! )(?=[.,!?()"\'/:;/\\\[^\]])|(?<=[.,!?()"\'/:;/\\\[^\]])(?! )', r' ', rev)
  rev = re.sub(r'[.,!?()"\'/:;/\\\-]', ' ', rev)
  rev = re.sub(r'\s+', ' ', rev)
  # stop_words = set(stopwords.words('romanian'))
  # words=rev.split()
  # words = [word for word in words if word.lower() not in stop_words]
  # rev = ' '.join(filtered_words)
  words=rev.split()
  words = filter_alphabetic_tokens(words)
  rev=' '.join(words)
  # words= [word for word in words if word not in stoppp]
  doc = nlp(rev)
  lemmatized_tokens = [word.lemma for sent in doc.sentences for word in sent.words]
  # print(lemmatized_tokens)
  lemmatized_tokens=[token for token in lemmatized_tokens if isinstance(token, str)]
  lemmatized_tokens=[token for token in lemmatized_tokens if not token.isnumeric()]
  lemmatized_tokens=[token for token in lemmatized_tokens if not any(char.isdigit() for char in token)]
  clean_voc.update(lemmatized_tokens)
  # lemmatized_tokens=[token for token in lemmatized_tokens if token in clean_voc]
  # print (token for token in lemmatized_tokens if not any(char.isdigit() for char in token))
  # for token in lemmatized_tokens:
  #   if any(char.isdigit() for char in token):
  #     print(token)
    # print(token)
    # print(token)
  rev=' '.join(lemmatized_tokens)
  return rev

def read_train():
  train_pos=list()
  train_neg=list()
  train_star3=list()
  for i in train_data['reviews']:
    # print(i)
    match i['starRating']:
      case '1':
        train_neg.append(i)
      case '2':
        train_neg.append(i)
      case '3':
        train_star3.append(i)
      case '4':
        train_pos.append(i)
      case '5':
        train_pos.append(i)

  train_pos_clean=list()
  for i in train_pos:
    # print(i)
    rev=clean_line(i)
    train_pos_clean.append(rev)

  train_neg_clean=list()
  for i in train_neg:
    rev=clean_line(i)
    train_neg_clean.append(rev)

# file_path = 'vocab_clean.txt'

def write_vocab(file_path,vocab):
  # Write each tuple to the file
  with open(file_path, 'w') as file:
      for word, count in vocab.items():
          file.write(f'{word} {count}\n')
def write_dset(file_path,dset):
  with open(file_path, 'w') as file:
    json.dump(list(dset), file)


def read_test():
  test_pos=list()
  test_neg=list()
  test_star3=list()
  for i in test_data['reviews']:
    # print(i)
    match i['starRating']:
      case '1':
        test_neg.append(i)
      case '2':
        test_neg.append(i)
      case '3':
        test_star3.append(i)
      case '4':
        test_pos.append(i)
      case '5':
        test_pos.append(i)

  test_pos_clean=list()
  for i in test_pos:
    rev=clean_line(i)

    test_pos_clean.append(rev)

  test_neg_clean=list()
  for i in test_neg:
    rev=clean_line(i)
    test_neg_clean.append(rev)

def write_dsets():
  write_dset('train_neg.json',train_neg_clean)
  write_dset('test_neg.json',test_neg_clean)
  write_dset('train_pos.json',train_pos_clean)
  write_dset('test_pos.json',test_pos_clean)

def get_dsets():
  file_path = 'train_pos.json'
  train_pos_clean=read_dset(file_path)

  file_path = 'train_neg.json'
  train_neg_clean=read_dset(file_path)

  file_path = 'test_pos.json'
  test_pos_clean=read_dset(file_path)

  file_path = 'test_neg.json'
  test_neg_clean=read_dset(file_path)

  # print(len(train_pos_clean))
  # print(train_pos_clean)

  # print(len(train_neg_clean))
  # print(train_neg_clean)
  #
  # print(len(test_pos_clean))
  # print(test_pos_clean)
  #
  # print(len(test_neg_clean))
  # print(test_neg_clean)

  return train_pos_clean, train_neg_clean, test_pos_clean, test_neg_clean

vocab_pos, vocab_neg, vocab_pos_nlemm, vocab_neg_nlemm, clean_voc = get_vocabs()
train_pos_clean, train_neg_clean, test_pos_clean, test_neg_clean =get_dsets()

def write_vocabs():
  file_path = 'vocab_neg.txt'

  # Write each tuple to the file
  with open(file_path, 'w') as file:
      for word, count in vocab_neg.items():
          file.write(f'{word} {count}\n')

  file_path = 'vocab_pos.txt'

  # Write each tuple to the file
  with open(file_path, 'w') as file:
      for word, count in vocab_pos.items():
          file.write(f'{word} {count}\n')

  file_path = 'vocab_neg_nlemm.txt'

  # Write each tuple to the file
  with open(file_path, 'w') as file:
      for word, count in vocab_neg_nlemm.items():
          file.write(f'{word} {count}\n')

  file_path = 'vocab_pos_nlemm.txt'

  # Write each tuple to the file
  with open(file_path, 'w') as file:
      for word, count in vocab_pos_nlemm.items():
          file.write(f'{word} {count}\n')


print(vocab_pos.most_common(50))
print(vocab_neg.most_common(50))

dict_pos = dict(vocab_pos)
dict_neg = dict(vocab_neg)

# Merge dictionaries and handle occurrences
merged_dict = {}
for word in set(dict_pos.keys()).union(dict_neg.keys()):
    occurrences_pos = dict_pos.get(word, 0)
    occurrences_neg = dict_neg.get(word, 0)
    merged_dict[word] = (occurrences_pos, occurrences_neg)


# Order the merged dictionary by maximum occurrence
sorted_merged_dict = dict(sorted(merged_dict.items(), key=lambda item: sum(item[1]), reverse=True))


percentage_difference = {
    word: ((counts[0] - counts[1]) / ((counts[0] + counts[1]) / 2)) * 100
    for word, counts in sorted_merged_dict.items()
}

# Display the first 50 items in the sorted merged dictionary with percentage difference
for i, (word, counts) in enumerate(sorted_merged_dict.items()):
    if i < 50:
        diff_percentage = percentage_difference[word]
        # print(f'{word}: {counts[0]} (positive), {counts[1]} (negative), Percentage Difference: {diff_percentage:.2f}%')
    else:
        break


def load_doc(filename):
	# open the file as read only
	file = open(filename, 'r')
	# read all text
	text = file.read()
	# close the file
	file.close()
	return text



def load_vocs():
  vocab_filename = 'vocab_pos.txt'
  vocab_pos = load_doc(vocab_filename)
  # vocab = tokens
  vocab_pos = vocab_pos.split()
  vocab_pos = set(vocab_pos)

  vocab_filename = 'vocab_neg.txt'
  vocab_neg = load_doc(vocab_filename)
  # vocab = tokens
  vocab_neg = vocab_neg.split()
  vocab_neg = set(vocab_neg)

def remove_vocab(sett,rvocab):
  nset=list()
  for doc in sett:
    tokens = doc.split()
    tokens = [w for w in tokens if w not in rvocab]
    tokens = ' '.join(tokens)
    nset.append(tokens)
  return nset



def rem_words(perc):
  c=0

  wordds=[]

  for word in stop_words:
    lem=nlp(word)
    lem=lem.sentences[0]
    lem=lem.words[0]
    lem=lem.lemma
    if lem in sorted_merged_dict :
      if abs(percentage_difference[lem])>perc and sum(sorted_merged_dict[lem])>2:
        # print(lem+' '+word+ ' '+str(percentage_difference[lem]))
        c+=1;
        wordds.append(word)

  print(c)
  # print(wordds)
  return wordds


def train_prep(test_train_neg_clean,test_train_pos_clean):
  train_feat = np.array(test_train_neg_clean+test_train_pos_clean)
  train_lab = np.array([0 for _ in range(6000)] + [1 for _ in range(6000)])

  train = pd.DataFrame([train_feat, train_lab]).T
  train.columns = ['data', 'label']
  # train['DATA_COLUMN'] = train['DATA_COLUMN'].str.decode("utf-8")
  return train

def test_prep(test_test_neg_clean,test_test_pos_clean):
  test_feat = np.array(test_test_neg_clean+test_test_pos_clean)
  test_lab = np.array([0 for _ in range(1500)] + [1 for _ in range(1500)])

  test = pd.DataFrame([test_feat, test_lab]).T
  test.columns = ['data', 'label']
  # train['DATA_COLUMN'] = train['DATA_COLUMN'].str.decode("utf-8")
  return test

print(len(stop_words))



def test_bayes(btrain,btest):
  # Extract data and labels from the training set
  train_texts = btrain['data'].tolist()
  train_labels = btrain['label'].tolist()

  # Extract data and labels from the test set
  test_texts = btest['data'].tolist()
  test_labels = btest['label'].tolist()

  # Create a CountVectorizer to convert text data into a bag-of-words representation
  vectorizer = CountVectorizer()
  X_train = vectorizer.fit_transform(train_texts)
  X_test = vectorizer.transform(test_texts)

  # Create and train the Multinomial Naive Bayes classifier
  nb_classifier = MultinomialNB()
  nb_classifier.fit(X_train, train_labels)

  # Make predictions on the test set
  predictions = nb_classifier.predict(X_test)

  # Evaluate the performance
  accuracy = accuracy_score(test_labels, predictions)
  print(f"Accuracy: {accuracy:.2f}")

  # Print classification report
  print("Classification Report:")
  print(classification_report(test_labels, predictions))

def run_bayes():
  # print(clean_voc.most_common(50))
  tokens= [k for k,c in clean_voc.items() if c < 3]
  # print(tokens)
  for n in range(10):
    print("run--"+str(n*10))
    to_remove=tokens+rem_words(n*10)
    print(" to remove-"+str(len(to_remove))+' '+str(len(to_remove)-7275))
    test_train_neg_clean=remove_vocab(train_neg_clean,to_remove)
    test_train_pos_clean=remove_vocab(train_pos_clean,to_remove)
    test_test_neg_clean=remove_vocab(test_neg_clean,to_remove)
    test_test_pos_clean=remove_vocab(test_pos_clean,to_remove)
    print(" cleaned")
    train=train_prep(test_train_neg_clean,test_train_pos_clean)
    test=test_prep(test_test_neg_clean,test_test_pos_clean)
    print(" prepped")
    test_bayes(train,test)
    print(" done")


tokenizer = Tokenizer()

def prep_cnn(train_docs,test_docs,max_length):
  # train_docs=train_neg_clean+train_pos_clean
  tokenizer.fit_on_texts(train_docs)

  encoded_docs = tokenizer.texts_to_sequences(train_docs)

  # max_length = max([len(s.split()) for s in train_docs])

  xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
  ytrain = array([0 for _ in range(6000)] + [1 for _ in range(6000)])

  # test_docs=test_neg_clean+test_pos_clean
  encoded_docs = tokenizer.texts_to_sequences(test_docs)

  xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
  ytest = array([0 for _ in range(1500)] + [1 for _ in range(1500)])

  # define vocabulary size (largest integer value)
  vocab_size = len(tokenizer.word_index) + 1

  print(' len'+str(max_length))

  model = Sequential()
  model.add(Embedding(vocab_size, 100, input_length=max_length))
  model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))
  model.add(MaxPooling1D(pool_size=2))
  model.add(Flatten())
  model.add(Dense(10, activation='relu'))
  model.add(Dense(1, activation='sigmoid'))
  # print(model.summary())

  return xtrain, ytrain, xtest, ytest, model

w2v_model = Word2Vec.load("word2vec_model.bin")

def prep_cnn_w2v(train_docs,test_docs):


  def filter_words(review):
        return [word for word in review.split() if word in w2v_model.wv]

  train_docs = [filter_words(review) for review in train_docs]
  test_docs = [filter_words(review) for review in test_docs]

  embedded_reviews = [[w2v_model.wv[word] for word in review] for review in train_docs]
  # Pad sequences to a fixed length
  max_length = 200  # Adjust as needed based on your dataset
  xtrain = pad_sequences(embedded_reviews, maxlen=max_length, padding='post', truncating='post')
  ytrain = array([0 for _ in range(6000)] + [1 for _ in range(6000)])

  embedded_reviews = [[w2v_model.wv[word] for word in review] for review in test_docs]
  # Pad sequences to a fixed length
  max_length = 200  # Adjust as needed based on your dataset
  xtest = pad_sequences(embedded_reviews, maxlen=max_length, padding='post', truncating='post')
  ytest = array([0 for _ in range(1500)] + [1 for _ in range(1500)])

  # define vocabulary size (largest integer value)
  vocab_size = len(tokenizer.word_index) + 1

  model = Sequential()
  model.add(Conv1D(128, 3, activation='relu', input_shape=(max_length, 300)))  # Adjust filters and kernel_size as needed
  model.add(GlobalMaxPooling1D())
  model.add(Dense(128, activation='relu'))
  model.add(Dense(1, activation='sigmoid'))  # Binary classification
  # print(model.summary())

  return xtrain, ytrain, xtest, ytest, model

def test_cnn(xtrain, ytrain, xtest, ytest, model):
  # compile network
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
  # fit network
  model.fit(xtrain, ytrain, epochs=10, verbose=2)

  loss, acc = model.evaluate(xtest, ytest, verbose=0)
  print('Test Accuracy: %f' % (acc*100))
  return acc*100

def run_cnn(w2v):
  # print(clean_voc.most_common(50))
  tokens= [k for k,c in clean_voc.items() if c < 3]
  # print(tokens)
  acc=[]
  for n in range(10):
    # print("run--"+str(n*10))
    to_remove=tokens+rem_words(0)
    # print(" to remove-"+str(len(to_remove))+' '+str(len(to_remove)-7275))
    test_train_neg_clean=remove_vocab(train_neg_clean,to_remove)
    test_train_pos_clean=remove_vocab(train_pos_clean,to_remove)
    test_test_neg_clean=remove_vocab(test_neg_clean,to_remove)
    test_test_pos_clean=remove_vocab(test_pos_clean,to_remove)
    print(" cleaned")
    train=test_train_neg_clean + test_train_pos_clean
    test=test_test_neg_clean + test_test_pos_clean
    if w2v:
      xtrain, ytrain, xtest, ytest, model= prep_cnn_w2v(train,test)
    else:
      xtrain, ytrain, xtest, ytest, model= prep_cnn(train,test,(n+1)*100)
    print(" prepped")
    acc.append(test_cnn(xtrain, ytrain, xtest, ytest, model))
    print(" done")
  return acc


def evaluate(history,x_test):
  # Plot training and validation curves
  plt.plot(history.history['loss'], label='Training Loss')
  plt.plot(history.history['val_loss'], label='Validation Loss')
  plt.title('Training and Validation Loss')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.legend()
  plt.show()

  plt.plot(history.history['accuracy'], label='Training Accuracy')
  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
  plt.title('Training and Validation Accuracy')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.legend()
  plt.show()

  # Predict probabilities for each class
  y_pred_prob = model.predict(x_test)

  # Threshold probabilities to get predicted classes
  y_pred = (y_pred_prob > 0.5).astype(int)  # Assuming binary classification with a threshold of 0.5

  # Confusion matrix
  conf_matrix = confusion_matrix(y_test, y_pred)
  print("Confusion Matrix:")
  print(conf_matrix)

  # Classification report
  print("Classification Report:")
  print(classification_report(y_test, y_pred))

  # ROC curve and AUC
  fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
  roc_auc = auc(fpr, tpr)
  plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
  plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('Receiver Operating Characteristic (ROC) Curve')
  plt.legend(loc="lower right")
  plt.show()

def prep_dnn(train_docs,test_docs):
  # train_docs=train_neg_clean+train_pos_clean
  tokenizer.fit_on_texts(train_docs)

  encoded_docs = tokenizer.texts_to_sequences(train_docs)

  max_length = 200

  xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
  ytrain = array([0 for _ in range(6000)] + [1 for _ in range(6000)])

  # test_docs=test_neg_clean+test_pos_clean
  encoded_docs = tokenizer.texts_to_sequences(test_docs)

  xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
  ytest = array([0 for _ in range(1500)] + [1 for _ in range(1500)])

  # define vocabulary size (largest integer value)
  vocab_size = len(tokenizer.word_index) + 1

  print(' len'+str(max_length))

  model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length),
    Flatten(),  # Flatten the 3D embedding tensor into a 2D tensor
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
  ])

  return xtrain, ytrain, xtest, ytest, model

def run_dnn(w2v):
  # print(clean_voc.most_common(50))
  tokens= [k for k,c in clean_voc.items() if c < 3]
  # print(tokens)
  acc=[]
  for n in range(10):
    print("run--"+str(n*10))
    to_remove=tokens+rem_words(n*10)
    # print(" to remove-"+str(len(to_remove))+' '+str(len(to_remove)-7275))
    test_train_neg_clean=remove_vocab(train_neg_clean,to_remove)
    test_train_pos_clean=remove_vocab(train_pos_clean,to_remove)
    test_test_neg_clean=remove_vocab(test_neg_clean,to_remove)
    test_test_pos_clean=remove_vocab(test_pos_clean,to_remove)
    print(" cleaned")
    train=test_train_neg_clean + test_train_pos_clean
    test=test_test_neg_clean + test_test_pos_clean
    # if w2v:
    #   xtrain, ytrain, xtest, ytest, model= prep_cnn_w2v(train,test)
    # else:
    xtrain, ytrain, xtest, ytest, model= prep_dnn(train,test)
    print(" prepped")
    acc.append(test_cnn(xtrain, ytrain, xtest, ytest, model))
    print(acc)
    print(" done")

  return acc

# brek it don no

y_train = array([0 for _ in range(6000)] + [1 for _ in range(6000)])
y_test = array([0 for _ in range(1500)] + [1 for _ in range(1500)])


tokens= [k for k,c in clean_voc.items() if c < 3]
# print(tokens)
# acc=[]
  # for n in range(10):
  # print("run--"+str(n*10))
to_remove=tokens+rem_words(5*10)

test_train_neg_clean=remove_vocab(train_neg_clean,to_remove)
test_train_pos_clean=remove_vocab(train_pos_clean,to_remove)
test_test_neg_clean=remove_vocab(test_neg_clean,to_remove)
test_test_pos_clean=remove_vocab(test_pos_clean,to_remove)
print(" cleaned")
train=test_train_neg_clean + test_train_pos_clean
test=test_test_neg_clean + test_test_pos_clean

tagged_train_docs = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(train)]
tagged_test_docs = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(test)]

# Initialize and train the Doc2Vec model
doc2vec_model = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=20)
doc2vec_model.build_vocab(tagged_train_docs)
doc2vec_model.train(tagged_train_docs, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)

word2vec_model = Word2Vec.load('word2vec_model.bin')
def filter_words(review):
  return [word for word in review.split() if word in w2v_model.wv]

vocab_size = len(word2vec_model.wv)
embedding_dim = word2vec_model.vector_size

train_docs = [filter_words(review) for review in train]
test_docs = [filter_words(review) for review in test]

embedded_reviews = [[w2v_model.wv[word] for word in review] for review in train_docs]
xtrain = pad_sequences(embedded_reviews, maxlen=200, padding='post', truncating='post').astype('float32')

embedded_reviews = [[w2v_model.wv[word] for word in review] for review in test_docs]
xtest = pad_sequences(embedded_reviews, maxlen=200, padding='post', truncating='post').astype('float32')




"""-------------------------"""

def average_word_vectors(words, model, num_features):
    feature_vector = np.zeros((num_features,), dtype="float32")
    n_words = 0
    for word in words:
        if word in model.wv:
            feature_vector = np.add(feature_vector, model.wv[word])
            n_words += 1
    if n_words > 0:
        feature_vector = np.divide(feature_vector, n_words)
    return feature_vector

# Convert each document or sentence into an averaged word vector
def document_vectorizer(docs, model, num_features):
    doc_vectors = [average_word_vectors(doc, model, num_features) for doc in docs]
    return np.array(doc_vectors)

def shuffle(test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean):
  pos = test_train_pos_clean + test_test_pos_clean
  neg = test_train_neg_clean + test_test_neg_clean

  all_texts = pos + neg
  labels = [1] * len(pos) + [0] * len(neg)  # 1 for positive, 0 for negative

  # Split the data into training and testing sets
  train_texts, test_texts, train_labels, test_labels = train_test_split ( all_texts, labels, test_size=0.2, random_state=42)

  return train_texts, test_texts


def average_document_vectors(docs, dim):
  averaged_docs = []

  for doc in docs:
    # print(doc.shape)
    # doc.tolist()
    # print(len(doc))
    # print(len(doc[0]))
    if doc:  # Check if the document is not empty
      averaged_vector = np.mean(doc, axis=dim)
      averaged_docs.append(averaged_vector)
    else:
      # Handle empty documents (e.g., by skipping or providing default values)
      averaged_docs.append(
        np.zeros((word2vec_model.vector_size), dtype=np.float32))  # Example: Use zero vector for empty document
  return np.array(averaged_docs)

def run_cml(once,feat,dim,model,test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean):
  print(once,' ',feat, ' ', dim ,' ',model)
  # print(clean_voc.most_common(50))
  tokens= [k for k,c in clean_voc.items() if c < 3]
  # print(tokens)
  # acc=[]
  if once:
    # print("run--50")
    # to_remove=tokens+rem_words(50)
    # # print(" to remove-"+str(len(to_remove))+' '+str(len(to_remove)-7275))
    # test_train_neg_clean=remove_vocab(train_neg_clean,to_remove)
    # test_train_pos_clean=remove_vocab(train_pos_clean,to_remove)
    # test_test_neg_clean=remove_vocab(test_neg_clean,to_remove)
    # test_test_pos_clean=remove_vocab(test_pos_clean,to_remove)
    # print(" cleaned")
    train=test_train_neg_clean + test_train_pos_clean
    test=test_test_neg_clean + test_test_pos_clean
    # train, test= shuffle(test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
    ran=1
  else:
    ran=10
  for n in range(ran):
    if not once:
      print("run--"+str(n*10))
      to_remove=tokens+rem_words(n*10)
      # print(" to remove-"+str(len(to_remove))+' '+str(len(to_remove)-7275))
      test_train_neg_clean=remove_vocab(train_neg_clean,to_remove)
      test_train_pos_clean=remove_vocab(train_pos_clean,to_remove)
      test_test_neg_clean=remove_vocab(test_neg_clean,to_remove)
      test_test_pos_clean=remove_vocab(test_pos_clean,to_remove)
      print(" cleaned")
      train=test_train_neg_clean + test_train_pos_clean
      test=test_test_neg_clean + test_test_pos_clean
      # train, test= shuffle(test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)

    match feat:
      case "bow":
        vectorizer = CountVectorizer()
        X_train = vectorizer.fit_transform(train)
        X_test = vectorizer.transform(test)
        X_train=X_train.toarray()
        X_test=X_test.toarray()
      case "tfidf":
        tfidf_vectorizer = TfidfVectorizer()
        X_train = tfidf_vectorizer.fit_transform(train)
        X_test = tfidf_vectorizer.transform(test)

      case "w2v":
        vocab_size = len(word2vec_model.wv)
        embedding_dim = word2vec_model.vector_size

        train_docs = [filter_words(review) for review in train]
        test_docs = [filter_words(review) for review in test]

        X_train = [[w2v_model.wv[word] for word in review] for review in train_docs]
        # X_train = pad_sequences(embedded_reviews, maxlen=200, padding='post', truncating='post').astype('float32')

        X_test = [[w2v_model.wv[word] for word in review] for review in test_docs]
        # X_test = pad_sequences(embedded_reviews, maxlen=200, padding='post', truncating='post').astype('float32')

        X_train = average_document_vectors(X_train)
        X_test = average_document_vectors(X_test)

        # X_train = document_vectorizer(train, w2v_model, num_features=w2v_model.vector_size)
        # X_test =document_vectorizer(test, w2v_model, num_features=w2v_model.vector_size)

      case "d2v":
        # Prepare tagged documents for training Doc2Vec
        tagged_train_docs = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(train)]
        tagged_test_docs = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(test)]
        X_train = np.array([doc2vec_model.infer_vector(doc.words) for doc in tagged_train_docs])
        X_test = np.array([doc2vec_model.infer_vector(doc.words) for doc in tagged_test_docs])


    match dim:
      case "pca":
        pca = PCA(n_components=100)  # Adjust n_components as needed

        if feat=="tfidf":
          X_train=X_train.toarray()
          X_test=X_test.toarray()
        # Fit PCA on TF-IDF transformed training data and transform training/test data
        X_train = pca.fit_transform(X_train)
        X_test = pca.transform(X_test)

      case "nmf":
        nmf = NMF(n_components=10, random_state=42)
        if not feat == "tfidf":
          if np.any(X_train < 0) or np.any(X_test < 0):
            offset = np.abs(np.min(X_train)) + 1
            X_train += offset
            X_test += offset

        X_train = nmf.fit_transform(X_train)
        X_test = nmf.transform(X_test)

      case "lsa":
        lsa = TruncatedSVD(n_components=100)

        X_train = lsa.fit_transform(X_train)
        X_test = lsa.transform(X_test)

      case "nonee":
        print('none')

      # case _:
      #   print("nyumere1")
      #   break
    # X_train_tfidf = tfidf_vectorizer.fit_transform(train)
    # X_test_tfidf = tfidf_vectorizer.transform(test)
    # print(X_train_tfidf[1],'-------------------------')
    match model:
      case "mnb":
        print("mere2")
        if not (feat == "tfidf" and dim=="nonee"):
          if np.any(X_train < 0) or np.any(X_test < 0):
            # Add a constant offset to ensure non-negative values
            offset = np.abs(np.min(X_train)) + 1  # Use a constant offset slightly larger than the minimum value
            X_train += offset
            X_test += offset
        model=MultinomialNB()
      case "bnb":
        print("mere2")
        if not (feat == "tfidf" and dim=="nonee"):
          if np.any(X_train < 0) or np.any(X_test < 0):
            # Add a constant offset to ensure non-negative values
            offset = np.abs(np.min(X_train)) + 1  # Use a constant offset slightly larger than the minimum value
            X_train += offset
            X_test += offset
        model=BernoulliNB()
      case "lr":
        model=LogisticRegression()
      case "rf":
        model=RandomForestClassifier(n_estimators=100, random_state=42)
      case "svm":
        model=LinearSVC()
      case "knn":
        model=KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='euclidean')
      case "wknn":
        model=KNeighborsClassifier(n_neighbors=5, weights='distance', metric='euclidean')
      case "dnn":
        vocab_size = len(tfidf_vectorizer.vocabulary_)
        X_train_tfidf_reordered = tf.sparse.reorder(tf.sparse.SparseTensor(X_train.indices, X_train.values, X_train.dense_shape))
        X_test_tfidf_reordered = tf.sparse.reorder(tf.sparse.SparseTensor(X_test.indices, X_test.values, X_test.dense_shape))

        model=Sequential([
            Embedding(input_dim=X_train_tfidf_reordered.shape[1], output_dim=100, input_length=X_train_tfidf_reordered.shape[1]),
            Flatten(),  # Flatten the 3D embedding tensor into a 2D tensor
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(64, activation='relu'),
            Dropout(0.5),
            Dense(1, activation='sigmoid')
        ])
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        X_train = tf.sparse.to_dense(X_train_tfidf_reordered)
        X_test = tf.sparse.to_dense(X_test_tfidf_reordered)

      case _:
        print("numere2")
        break


    y_train = array([0 for _ in range(6000)] + [1 for _ in range(6000)])
    y_test = array([0 for _ in range(1500)] + [1 for _ in range(1500)])

    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_test)
    print(classification_report(y_test, y_pred))
    acc = accuracy_score(y_test, y_pred)

  return acc

import joblib
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier


def run_opt_rf():

  # Define the model
  rf = RandomForestClassifier(random_state=42)

  # Define the parameter grid

  train=test_train_neg_clean + test_train_pos_clean
  test=test_test_neg_clean + test_test_pos_clean

  vectorizer = CountVectorizer()
  X_train = vectorizer.fit_transform(train)
  X_test = vectorizer.transform(test)
  # X_train=X_train.toarray()
  # X_test=X_test.toarray()

  y_train = array([0 for _ in range(6000)] + [1 for _ in range(6000)])
  y_test = array([0 for _ in range(1500)] + [1 for _ in range(1500)])


  param_grid = {
      'n_estimators': [100, 200, 500],
      'max_depth': [None, 10, 20, 30],
      'min_samples_split': [2, 5, 10],
      'min_samples_leaf': [1, 2, 4],
      'max_features': [None, 'sqrt', 'log2']
  }

  # Setup the grid search using the loky backend explicitly
  # with joblib.parallel_backend('loky'):
  #     grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
  #     grid_search.fit(X_train, y_train)

  with joblib.parallel_backend('loky'):
      grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
      grid_search.fit(X_train, y_train)

  # # Setup the grid search
  # grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

  # # Fit the model to the data
  # grid_search.fit(X_train, y_train)

  # Get the best parameters
  best_params = grid_search.best_params_
  print(f"Best parameters for RandomForest: {best_params}")

  num_processors = os.cpu_count()
  print(f"Number of processors available: {num_processors}")

  best_accuracy = grid_search.best_score_
  print(f"Best accuracy achieved during grid search: {best_accuracy:.4f}")

# run_opt_rf()

def run_opt_lr():
    model = LogisticRegression()

    # Define the parameter grid
    param_grid = {
        'penalty': ['l1', 'l2', 'elasticnet', None],
        'C': [0.01, 0.1, 1, 10, 100],
        'solver': ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga'],
        'max_iter': [100, 200, 300],
        'tol': [1e-4, 1e-3, 1e-2],
        'fit_intercept': [True, False],
        'l1_ratio': [0.1, 0.5, 0.9],  # Only relevant if penalty='elasticnet'
        'dual': [True, False],  # Only relevant if solver='liblinear'
        'intercept_scaling': [1, 2, 5],  # Only relevant if solver='liblinear'
        # 'verbose' :['3']
    }

    opt_set = test_train_neg_clean + test_test_neg_clean + test_test_pos_clean + test_train_pos_clean

    y_opt = np.array([0 for _ in range(7500)] + [1 for _ in range(7500)])

    vectorizer = CountVectorizer()
    X_opt = vectorizer.fit_transform(opt_set)

    # Setup the grid search
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)

    # Fit the grid search to the data
    grid_search.fit(X_opt, y_opt)

    # Print the best parameters and best score
    print("Best parameters found: ", grid_search.best_params_)
    print("Best accuracy score: ", grid_search.best_score_)

import optuna
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from scipy.sparse import issparse

opt_set = test_train_neg_clean + test_test_neg_clean + test_test_pos_clean + test_train_pos_clean

y_opt = np.array([0 for _ in range(7500)] + [1 for _ in range(7500)])

vectorizer = CountVectorizer()
X_opt = vectorizer.fit_transform(opt_set)


def objective(trial):
  penalty = trial.suggest_categorical('penalty', [None, 'l1', 'l2', 'elasticnet'])
  C = trial.suggest_float('C', 1e-5, 1e5, log=True)
  solver = trial.suggest_categorical('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])
  max_iter = trial.suggest_int('max_iter', 100, 1000)
  tol = trial.suggest_float('tol', 1e-5, 1e-1, log=True)
  fit_intercept = trial.suggest_categorical('fit_intercept', [True, False])

  if penalty == 'elasticnet':
    l1_ratio = trial.suggest_float('l1_ratio', 0.0, 1.0)
  else:
    l1_ratio = None

  # Ensure the combination of parameters is valid
  if solver in ['lbfgs', 'newton-cg', 'newton-cholesky', 'sag']:
    # 'lbfgs', 'newton-cg', 'newton-cholesky', and 'sag' support only 'l2' or None penalties
    if penalty not in ['l2', None]:
      penalty = 'l2'  # Default to 'l2' if invalid

  if solver == 'liblinear':
    # 'liblinear' supports 'l1' and 'l2' penalties
    if penalty not in ['l1', 'l2']:
      penalty = 'l2'  # Default to 'l2' if invalid
    dual = False  # 'liblinear' does not support dual=True for 'l1' penalty

  if solver == 'saga':
    # 'saga' supports 'elasticnet', 'l1', 'l2', and None penalties
    pass  # No action needed here as all penalty options are valid for 'saga'

  # For other solvers, dual is not a parameter
  if solver not in ['liblinear']:
    dual = False

  # Adjust preprocessing based on data sparsity
  if issparse(X_opt):
    scaler = StandardScaler(with_mean=False)  # Do not center sparse matrices
  else:
    scaler = StandardScaler()

  # Build the pipeline
  pipeline = make_pipeline(
    scaler,
    LogisticRegression(
      penalty=penalty,
      C=C,
      solver=solver,
      max_iter=max_iter,
      tol=tol,
      fit_intercept=fit_intercept,
      dual=dual,
      l1_ratio=l1_ratio
    )
  )

  # Evaluate the model using cross-validation
  score = cross_val_score(pipeline, X_opt, y_opt, cv=5, scoring='accuracy', n_jobs=-1).mean()

  return score

def run_opt_gauss_lr():
  # Assuming test_train_neg_clean, test_test_neg_clean, test_test_pos_clean, and test_train_pos_clean are already defined
  opt_set = test_train_neg_clean + test_test_neg_clean + test_test_pos_clean + test_train_pos_clean

  y_opt = np.array([0 for _ in range(7500)] + [1 for _ in range(7500)])

  vectorizer = CountVectorizer()
  X_opt = vectorizer.fit_transform(opt_set)

  # Create the Optuna study with Gaussian Process optimization
  study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())
  study.optimize(objective, n_trials=50)

  # Print the best parameters and best score
  rezult=(study.best_params,study.best_value)
  print("Best parameters found: ", study.best_params)
  print("Best accuracy score: ", study.best_value)
  return rezult



#aaaa

# param_grid = {
#       'n_estimators': [100, 200, 500],
#       'max_depth': [None, 10, 20, 30],
#       'min_samples_split': [2, 5, 10],
#       'min_samples_leaf': [1, 2, 4],
#       'max_features': [None, 'sqrt', 'log2']
#   }

def objective_rf(trial):
  n_features = X_opt.shape[1]
  rf = RandomForestClassifier(
    n_estimators=trial.suggest_int('n_estimators', 100, 500),  # Number of trees in the forest
    # criterion=trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss']),  # Splitting criterion
    max_depth=trial.suggest_categorical('max_depth', [None, 10, 20, 30]),  # Maximum depth of the tree
    min_samples_split=trial.suggest_int('min_samples_split', 2, 10),  # Minimum number of samples to split a node
    min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 10),  # Minimum number of samples required at a leaf node
    # min_weight_fraction_leaf=trial.suggest_uniform('min_weight_fraction_leaf', 0.0, 0.5), # Minimum weighted fraction of the sum total of weights for leaf nodes
    # max_features=trial.suggest_categorical('max_features', ['sqrt']), # Number of features to consider when looking for the best split
    # max_leaf_nodes=trial.suggest_int('max_leaf_nodes', 10, 1000),  # Maximum number of leaf nodes in the tree
    # min_impurity_decrease=trial.suggest_float('min_impurity_decrease', 0.0, 0.2), # Minimum decrease in impurity for a node split to occur
    # class_weight=trial.suggest_categorical('class_weight', [None, 'balanced', 'balanced_subsample']), # Weights associated with classes
    # ccp_alpha=trial.suggest_float('ccp_alpha', 0.0, 0.1), # Complexity parameter used for Minimal Cost-Complexity Pruning
    # max_samples=trial.suggest_float('max_samples', 0.5, 1.0), # Fraction of samples to draw for training each base estimator
    # monotonic_cst=trial.suggest_categorical('monotonic_cst', [None] + [[1] * n_features, [0] * n_features, [-1] * n_features]) # Monotonicity constraints
  )
  return cross_val_score(rf, X_opt, y_opt, n_jobs=-1, cv=5, verbose=3).mean()

import optuna
# from optuna.integration import SQLiteStorage

def run_opt_gauss_rf(study):
  if study is None:
    study = optuna.create_study(direction='maximize')

  study.optimize(objective_rf, n_trials=100, n_jobs=-1)

  rezult = (study.best_params, study.best_value)
  print("Best parameters found: ", study.best_params)
  print("Best accuracy score: ", study.best_value)
  return study


# rezultz=[]
#
# study=None
#
# for i in range(10):
#   study=run_opt_gauss_rf(None)
#   rezult = (study.best_params, study.best_value)
#   rezultz.append(rezult)
#
# for i in range(10):
#   print(rezultz[i])

from sklearn.svm import LinearSVC, SVC, NuSVC, OneClassSVM

import numpy as np
from sklearn.svm import NuSVC
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import optuna
from scipy.sparse import issparse

def objective_nusvc(trial):
    # Suggest values for hyperparameters
    nu = trial.suggest_float('nu', 0.01, 1.0)
    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])
    degree = trial.suggest_int('degree', 2, 10) if kernel == 'poly' else 3
    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])
    coef0 = trial.suggest_float('coef0', 0.0, 10.0) if kernel in ['poly', 'sigmoid'] else 0.0
    shrinking = trial.suggest_categorical('shrinking', [True, False])
    probability = trial.suggest_categorical('probability', [True, False])
    tol = trial.suggest_loguniform('tol', 1e-4, 1e-1)
    cache_size = trial.suggest_float('cache_size', 100.0, 1000.0)
    class_weight = trial.suggest_categorical('class_weight', [None, 'balanced'])
    verbose = trial.suggest_categorical('verbose', [True, False])
    max_iter = trial.suggest_int('max_iter', 100, 10000)
    decision_function_shape = trial.suggest_categorical('decision_function_shape', ['ovo', 'ovr'])
    break_ties = trial.suggest_categorical('break_ties', [True, False]) if decision_function_shape == 'ovr' and len(set(y_opt)) > 2 else False
    random_state = trial.suggest_int('random_state', 0, 10000)

    # Check if X_opt is sparse, and convert to dense if needed
    if issparse(X_opt):
        X_dense = X_opt.toarray()  # Convert sparse matrix to dense
    else:
        X_dense = X_opt

    # Check for NaN or infinite values in X_opt (dense array)
    if np.any(np.isnan(X_dense)) or np.any(np.isinf(X_dense)):
        raise ValueError("Input data contains NaN or infinite values. Please clean your data.")

    # Use a pipeline to standardize the data and fit the model
    pipeline = Pipeline([
        ('scaler', StandardScaler()),  # Standardize features if needed
        ('nusvc', NuSVC(
            nu=nu,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            coef0=coef0,
            shrinking=shrinking,
            probability=probability,
            tol=tol,
            cache_size=cache_size,
            class_weight=class_weight,
            verbose=verbose,
            max_iter=max_iter,
            decision_function_shape=decision_function_shape,
            break_ties=break_ties,
            random_state=random_state
        ))
    ])

    # Evaluate the model with cross-validation, catching potential numerical issues
    score = cross_val_score(pipeline, X_dense, y_opt, n_jobs=-1, cv=5, verbose=3, error_score='raise').mean()

    return score

def run_opt_nusvc(study=None):
    if study is None:
        study = optuna.create_study(direction='maximize')

    study.optimize(objective_nusvc, n_trials=100, n_jobs=-1)

    results = (study.best_params, study.best_value)
    print("Best parameters found: ", study.best_params)
    print("Best accuracy score: ", study.best_value)
    return study


import optuna
import tensorflow as tf
from tensorflow.keras import layers, models

X_opt = X_opt.toarray()

# Define the objective function

best_accuracy = -1
best_params = None
best_f1_score = None

from sklearn.metrics import f1_score
def objective_dnn(trial):
    global best_accuracy, best_params, best_f1_score
    # Define the model architecture
    model = models.Sequential()

    # Input layer
    model.add(layers.Input(shape=(X_opt.shape[1],)))  # Shape based on the number of features

    # Add layers based on trial parameters
    for i in range(trial.suggest_int('num_layers', 2, 6)):
        units = trial.suggest_int(f'units_l{i}', 32, 256)
        model.add(layers.Dense(units, activation='relu'))
        if trial.suggest_categorical(f'dropout_l{i}', [True, False]):
            model.add(layers.Dropout(rate=trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)))

    # Output layer
    model.add(layers.Dense(2, activation='softmax'))  # Assuming binary classification (2 classes)

    # Compile the model
    model.compile(
        optimizer=trial.suggest_categorical('optimizer', ['adam', 'sgd']),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )


    # Train the model
    history = model.fit(X_opt, y_opt, epochs=trial.suggest_int('epochs', 1, 10), validation_split=0.2, verbose=0)

    # Evaluate the model
    score = model.evaluate(X_opt, y_opt, verbose=0)

    # Log accuracy (this will be used for optimization)
    accuracy = score[1]

    # Calculate and log F1 score for additional insights
    y_pred_prob = model.predict(X_opt, verbose=0)
    y_pred = np.argmax(y_pred_prob, axis=1)
    f1 = f1_score(y_opt, y_pred)

    # Check if this trial has the best accuracy
    if accuracy > best_accuracy:
      best_accuracy = accuracy
      best_f1_score = f1
      best_params = trial.params

    return accuracy  # Return accuracy for optimization


def run_opt_dnn(study=None):
  global best_accuracy, best_params, best_f1_score

  # Reinitialize variables for each run
  best_accuracy = -1
  best_params = None
  best_f1_score = None

  if study is None:
    study = optuna.create_study(direction='maximize')
  study.optimize(objective_dnn, n_trials=50)

  results = (study.best_params, study.best_value)
  print("Best parameters found: ", study.best_params)
  print("Best accuracy score: ", study.best_value)
  print("Best F1 score: ", best_f1_score)
  return study

#
# # First run of the optimization
# study = run_opt_dnn()
#
# # Print results for multiple runs
# results_list = []
# for _ in range(5):
#   # best_accuracy = -1
#   # best_params = None
#   # best_f1_score = None
#   study = run_opt_dnn()
#   results_list.append({
#     'best_params': best_params,
#     'best_accuracy': best_accuracy,
#     'best_f1_score': best_f1_score
#   })
#
# # Print the final list of results
# for i, result in enumerate(results_list):
#   print(
#     f"Run {i + 1}: Best parameters: {result['best_params']}, Best accuracy: {result['best_accuracy']}, Best F1 score: {result['best_f1_score']}")
#-----------------------------------------------------------
# Variables to keep track of the best trial
best_accuracy = -1
best_params = None
best_f1_score = None


def objective_cnn(trial):
  global best_accuracy, best_params, best_f1_score

  # Define the model architecture
  model = models.Sequential()

  # Input layer
  input_dim = X_opt.shape[1]  # This is the length of the BoW feature vectors
  model.add(layers.Input(shape=(input_dim, 1)))  # Adding a channel dimension for Conv1D

  # Add convolutional layers based on trial parameters
  for i in range(trial.suggest_int('num_conv_layers', 1, 3)):
    filters = trial.suggest_int(f'filters_l{i}', 32, 128)
    kernel_size = trial.suggest_int(f'kernel_size_l{i}', 3, 5)
    model.add(layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same'))
    model.add(layers.MaxPooling1D(pool_size=2))
    if trial.suggest_categorical(f'dropout_l{i}', [True, False]):
      model.add(layers.Dropout(rate=trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)))

  # Flatten layer
  model.add(layers.Flatten())

  # Add dense layers
  for i in range(trial.suggest_int('num_dense_layers', 1, 2)):
    units = trial.suggest_int(f'units_l_dense{i}', 64, 256)
    model.add(layers.Dense(units, activation='relu'))
    if trial.suggest_categorical(f'dropout_dense_l{i}', [True, False]):
      model.add(layers.Dropout(rate=trial.suggest_float(f'dropout_dense_rate_l{i}', 0.1, 0.5)))

  # Output layer
  model.add(layers.Dense(2, activation='softmax'))  # Assuming binary classification (2 classes)

  # Compile the model
  model.compile(
    optimizer=trial.suggest_categorical('optimizer', ['adam', 'sgd']),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']  # Optimizing based on accuracy
  )

  # Train the model
  history = model.fit(X_opt, y_opt, epochs=trial.suggest_int('epochs', 1, 10), validation_split=0.2, verbose=0)

  # Evaluate the model on the training data (or validation split if you want to evaluate on unseen data)
  score = model.evaluate(X_opt, y_opt, verbose=0)

  # Log accuracy (this will be used for optimization)
  accuracy = score[1]

  # Calculate F1 score
  y_pred_prob = model.predict(X_opt, verbose=0)
  y_pred = np.argmax(y_pred_prob, axis=1)
  f1 = f1_score(y_opt, y_pred)

  # Check if this trial has the best accuracy
  if accuracy > best_accuracy:
    best_accuracy = accuracy
    best_f1_score = f1
    best_params = trial.params

  return accuracy  # Return accuracy for optimization


def run_opt_cnn(study=None):
  global best_accuracy, best_params, best_f1_score

  # Reinitialize variables for each run
  best_accuracy = -1
  best_params = None
  best_f1_score = None

  if study is None:
    study = optuna.create_study(direction='maximize')
  study.optimize(objective_cnn, n_trials=50)

  results = (study.best_params, study.best_value)
  print("Best parameters found: ", study.best_params)
  print("Best accuracy score: ", study.best_value)
  print("Best F1 score: ", best_f1_score)
  return study

#
# # Print results for multiple runs
# # results_list = []
# for _ in range(5):
#   study = run_opt_cnn()
#   results_list.append({
#     'best_params': best_params,
#     'best_accuracy': best_accuracy,
#     'best_f1_score': best_f1_score
#   })


train_docs = [filter_words(review) for review in opt_set]

X_opt_w2v = [[w2v_model.wv[word] for word in review] for review in train_docs]
# X_train = pad_sequences(embedded_reviews, maxlen=200, padding='post', truncating='post').astype('float32')

print(len(X_opt_w2v))
print(len(X_opt_w2v[0]))
print(len(X_opt_w2v[0][0]))


def average_all_word_vectors(X_w2v):
  # X_w2v shape: (15000, 200, 300)
  # We want to take the average across the last axis (300) to reduce it to (15000, 200)
  X_avg = np.mean(X_w2v, axis=2)  # Average along the last dimension
  return X_avg

results_list=[]
# X_opt=X_opt.toarray()

import optuna
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.metrics import f1_score

train_docs = [filter_words(review) for review in opt_set]

X_opt_w2v = [[w2v_model.wv[word] for word in review] for review in train_docs]
# X_train = pad_sequences(embedded_reviews, maxlen=200, padding='post', truncating='post').astype('float32')

# X_opt_w2v = pad_sequences(X_opt_w2v, maxlen=200, padding='post')

# X_opt_w2v.tolist()
X_opt_w2v_a = average_document_vectors(X_opt_w2v,0)

X_opt_w2v_p = pad_sequences(X_opt_w2v, maxlen=50, padding='post', truncating='post').astype('float32')

# X_opt_w2v_a = average_all_word_vectors(X_opt_w2v,1)

print(len(X_opt_w2v))
print(len(X_opt_w2v[0]))
print(len(X_opt_w2v[0][0]))

print(X_opt_w2v_a.shape)
print(X_opt_w2v_p.shape)

def objective_dnn_w2v(trial):
  global best_accuracy, best_params, best_f1_score
  # Define the model architecture
  model = models.Sequential()

  X_train = X_opt_w2v_a



  # num_features = X_train.shape[1] * X_train.shape[2]
  # X_train = X_train.reshape(X_train.shape[0], num_features)

  # Input layer
  model.add(layers.Input(shape=(X_train.shape[1],)))  # Shape based on the number of features

  # Add layers based on trial parameters
  for i in range(trial.suggest_int('num_layers', 2, 6)):
    units = trial.suggest_int(f'units_l{i}', 32, 256)
    model.add(layers.Dense(units, activation='relu'))
    if trial.suggest_categorical(f'dropout_l{i}', [True, False]):
      model.add(layers.Dropout(rate=trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)))

  # Output layer
  model.add(layers.Dense(2, activation='softmax'))  # Assuming binary classification (2 classes)

  # Compile the model
  model.compile(
    optimizer=trial.suggest_categorical('optimizer', ['adam', 'sgd']),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
  )

  # Train the model
  history = model.fit(X_train, y_opt, epochs=trial.suggest_int('epochs', 1, 10), validation_split=0.2, verbose=0)

  # Evaluate the model
  score = model.evaluate(X_train, y_opt, verbose=0)

  # Log accuracy (this will be used for optimization)
  accuracy = score[1]

  # Calculate and log F1 score for additional insights
  y_pred_prob = model.predict(X_opt, verbose=0)
  y_pred = np.argmax(y_pred_prob, axis=1)
  f1 = f1_score(y_opt, y_pred)

  # Check if this trial has the best accuracy
  if accuracy > best_accuracy:
    best_accuracy = accuracy
    best_f1_score = f1
    best_params = trial.params

  return accuracy  # Return accuracy for optimization

def run_opt_dnn_w2v(study=None):
  global best_accuracy, best_params, best_f1_score

  # Reinitialize variables for each run
  best_accuracy = -1
  best_params = None
  best_f1_score = None

  if study is None:
    study = optuna.create_study(direction='maximize')
  study.optimize(objective_dnn_w2v, n_trials=50)

  results = (study.best_params, study.best_value)
  print("Best parameters found: ", study.best_params)
  print("Best accuracy score: ", study.best_value)
  print("Best F1 score: ", best_f1_score)
  return study


#
# # First run of the optimization
# study = run_opt_dnn_w2v()
#
# # Print results for multiple runs
# results_list = []
# for _ in range(5):
#   # best_accuracy = -1
#   # best_params = None
#   # best_f1_score = None
#   study = run_opt_dnn_w2v()
#   results_list.append({
#     'best_params': best_params,
#     'best_accuracy': best_accuracy,
#     'best_f1_score': best_f1_score
#   })
#
# # Print the final list of results
# for i, result in enumerate(results_list):
#   print(
#     f"Run {i + 1}: Best parameters: {result['best_params']}, Best accuracy: {result['best_accuracy']}, Best F1 score: {result['best_f1_score']}")


import optuna
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split

# Variables to keep track of the best trial
best_accuracy = -1
best_params = None
best_f1_score = None

def objective_knn(trial):
    global best_accuracy, best_params, best_f1_score

    # Define the KNN model with trial-suggested hyperparameters
    n_neighbors = trial.suggest_int('n_neighbors', 1, 50)
    weights = trial.suggest_categorical('weights', ['uniform', 'distance'])
    algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])
    leaf_size = trial.suggest_int('leaf_size', 10, 50)
    p = trial.suggest_int('p', 1, 2)
    metric = trial.suggest_categorical('metric', ['minkowski', 'manhattan', 'euclidean', 'chebyshev'])

    # Set metric_params conditionally
    metric_params = {'p': p} if metric == 'minkowski' else None

    # Build model_kwargs
    model_kwargs = {
      'n_neighbors': n_neighbors,
      'weights': weights,
      'algorithm': algorithm,
      'leaf_size': leaf_size,
      'metric': metric,
    }

    # Add metric_params only if it's not None
    if metric_params is not None:
      model_kwargs.update(metric_params)

    # Initialize the KNeighborsClassifier
    model = KNeighborsClassifier(**model_kwargs)

    # Train-test split for evaluation
    X_train, X_test, y_train, y_test = train_test_split(X_opt, y_opt, test_size=0.2, random_state=42)

    # X_train = X_opt_w2v_a
    # y_train=y_opt

    # Train the model
    model.fit(X_train, y_train)

    # Evaluate the model
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')

    # Check if this trial has the best accuracy
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_f1_score = f1
        best_params = trial.params

    return accuracy  # Return accuracy for optimization

def run_opt_knn(study=None):
    global best_accuracy, best_params, best_f1_score

    # Reinitialize variables for each run
    best_accuracy = -1
    best_params = None
    best_f1_score = None

    if study is None:
        study = optuna.create_study(direction='maximize')
    study.optimize(objective_knn, n_trials=50, n_jobs=-1)

    results = (study.best_params, study.best_value)
    print("Best parameters found: ", study.best_params)
    print("Best accuracy score: ", study.best_value)
    print("Best F1 score: ", best_f1_score)
    return study
#
# # Print results for multiple runs
# results_list = []
# for _ in range(4):
#     study = run_opt_knn()
#     results_list.append({
#         'best_params': best_params,
#         'best_accuracy': best_accuracy,
#         'best_f1_score': best_f1_score
#     })
#     # Clear CUDA cache if using GPU, although KNN does not utilize GPUs
#     # torch.cuda.empty_cache() # Uncomment if using GPU for other parts
#
# # Print the final list of results
# for i, result in enumerate(results_list):
#     print(f"Run {i + 1}: Best parameters: {result['best_params']}, Best accuracy: {result['best_accuracy']}, Best F1 score: {result['best_f1_score']}")
#


best_accuracy = -1
best_params = None
best_f1_score = None


def objective_ocsvm(trial):
  global best_accuracy, best_params, best_f1_score

  # Define the One-Class SVM model with trial-suggested hyperparameters
  nu = trial.suggest_float('nu', 0.01, 1.0)  # Anomaly threshold
  kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])
  gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])

  # Hyperparameters specific to certain kernels
  degree = trial.suggest_int('degree', 2, 10) if kernel == 'poly' else 3
  coef0 = trial.suggest_float('coef0', 0.0, 10.0) if kernel in ['poly', 'sigmoid'] else 0.0

  shrinking = trial.suggest_categorical('shrinking', [True, False])
  tol = trial.suggest_float('tol', 1e-4, 1e-1)
  cache_size = trial.suggest_float('cache_size', 100.0, 1000.0)
  max_iter = trial.suggest_int('max_iter', 100, 10000)

  # Initialize the OneClassSVM
  model = OneClassSVM(
    nu=nu,
    kernel=kernel,
    gamma=gamma,
    degree=degree,
    coef0=coef0,
    shrinking=shrinking,
    tol=tol,
    cache_size=cache_size,
    max_iter=max_iter
  )

  # Train-test split for evaluation
  X_train, X_test, y_train, y_test = train_test_split(X_opt, y_opt, test_size=0.2, random_state=42)

  # Train the model (One-Class SVM does not use y_train)
  model.fit(X_train)

  # Predict anomalies (-1 for outliers, 1 for inliers)
  y_pred = model.predict(X_test)

  # Convert predictions: outliers as 0 and inliers as 1 for binary classification metrics
  y_pred = [1 if x == 1 else 0 for x in y_pred]
  y_test = [1 if x == 1 else 0 for x in y_test]

  # Evaluate the model
  accuracy = accuracy_score(y_test, y_pred)
  f1 = f1_score(y_test, y_pred, average='weighted')

  # Check if this trial has the best accuracy
  if accuracy > best_accuracy:
    best_accuracy = accuracy
    best_f1_score = f1
    best_params = trial.params

  return accuracy  # Return accuracy for optimization


def run_opt_ocsvm(study=None):
  global best_accuracy, best_params, best_f1_score

  # Reinitialize variables for each run
  best_accuracy = -1
  best_params = None
  best_f1_score = None

  if study is None:
    study = optuna.create_study(direction='maximize')
  study.optimize(objective_ocsvm, n_trials=50, n_jobs=-1)

  results = (study.best_params, study.best_value)
  print("Best parameters found: ", study.best_params)
  print("Best accuracy score: ", study.best_value)
  print("Best F1 score: ", best_f1_score)
  return study

#
# # Print results for multiple runs
# results_list = []
# for _ in range(4):
#   study = run_opt_ocsvm()
#   results_list.append({
#     'best_params': best_params,
#     'best_accuracy': best_accuracy,
#     'best_f1_score': best_f1_score
#   })
#
# # Print the final list of results
# for i, result in enumerate(results_list):
#   print(
#     f"Run {i + 1}: Best parameters: {result['best_params']}, Best accuracy: {result['best_accuracy']}, Best F1 score: {result['best_f1_score']}")



# Define the fixed parameters for Logistic Regression
fixed_lr_params = {
  'penalty': 'elasticnet',
  'C': 0.02,
  'solver': 'saga',
  'max_iter': 455,
  'tol': 0.002,
  'fit_intercept': True,
  'class_weight': None,
  'l1_ratio': 0.7,
  'dual': False,
  'intercept_scaling': 1.0
}


def objective_pca(trial):
  # PCA hyperparameters
  n_components = trial.suggest_int('pca_n_components', 2, min(X_opt.shape[0], X_opt.shape[1]))
  copy = trial.suggest_categorical('pca_copy', [True, False])
  whiten = trial.suggest_categorical('pca_whiten', [True, False])
  svd_solver = trial.suggest_categorical('pca_svd_solver', ['auto', 'full', 'arpack', 'randomized'])
  tol = trial.suggest_loguniform('pca_tol', 1e-5, 1e-1)
  iterated_power = trial.suggest_int('pca_iterated_power', 2, 7)
  power_iteration_normalizer = trial.suggest_categorical('pca_power_iteration_normalizer', ['auto', 'QR', 'LU'])

  # Create PCA and fixed LR pipeline
  pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Standardize features
    ('pca', PCA(n_components=n_components, copy=copy, whiten=whiten, svd_solver=svd_solver, tol=tol,
                iterated_power=iterated_power, power_iteration_normalizer=power_iteration_normalizer)),
    ('lr', LogisticRegression(
      penalty='elasticnet',
      C=0.02,
      solver='saga',
      max_iter=455,
      tol=0.002,
      fit_intercept=True,
      class_weight=None,
      l1_ratio=0.7,
      dual=False,
      intercept_scaling=1.0
    ))
  ])

  # Train-test split for evaluation
  X_train, X_test, y_train, y_test = train_test_split(X_opt, y_opt, test_size=0.2, random_state=42)

  # Train the model
  pipeline.fit(X_train, y_train)

  # Evaluate the model
  y_pred = pipeline.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)

  return accuracy


def run_optimization():
  # Create an Optuna study
  study = optuna.create_study(direction='maximize')

  # Optimize the objective function
  study.optimize(objective_pca, n_trials=50, n_jobs=-1)

  # Print the best parameters and score
  print("Best parameters found: ", study.best_params)
  print("Best accuracy score: ", study.best_value)

  # Rank trials by accuracy
  trials = study.trials
  trials_sorted = sorted(trials, key=lambda t: t.value, reverse=True)

  # Print all trials with their parameters and scores
  for i, trial in enumerate(trials_sorted):
    print(f"Rank {i + 1}: Trial {trial.number} - Accuracy: {trial.value:.4f}")
    print("  Parameters:", trial.params)

  return study


# Run the optimization
best_study = run_optimization()

stahp

# # Setup logging
# logging.basicConfig(filename='model_results.log', level=logging.INFO, format='%(asctime)s - %(message)s')

# # Function to run a single experiment and log the results
# def run_single_experiment(call):
#     feat, dim, model = call[1], call[2], call[3]

#     # Log the start of the experiment
#     logging.info(f"Starting: feat={feat}, dim={dim}, model={model}")

#     # Run the experiment
#     try:
#         accuracy = run_cml(*call)
#         logging.info(f"Finished: feat={feat}, dim={dim}, model={model} with accuracy={accuracy}")
#     except Exception as e:
#         logging.error(f"Error with feat={feat}, dim={dim}, model={model}: {e}")
#     return accuracy

def get_log_filename(base_name='experiment_log', ext='log'):
    counter = 1
    while True:
        filename = f"{base_name}_{counter}.{ext}"
        if not os.path.exists(filename):
            return filename
        counter += 1

import logging

# # Create a logger object
# logger = logging.getLogger('experiment_logger')
# logger.setLevel(logging.INFO)

# # Create a file handler for logging
# file_handler = logging.FileHandler('model_results.log')
# file_handler.setLevel(logging.INFO)

# # Create a logging format
# formatter = logging.Formatter('%(asctime)s - %(message)s')
# file_handler.setFormatter(formatter)

# # Add the handler to the logger
# logger.addHandler(file_handler)

# Function to run a single experiment and log the results
def run_single_experiment(call):
    feat, dim, model = call[1], call[2], call[3]
    accuracy=[]

    # Log the start of the experiment
    logger.info(f"Starting: feat={feat}, dim={dim}, model={model}")

    # Run the experiment
    try:
        accuracy = run_cml(*call)
        logger.info(f"Finished: feat={feat}, dim={dim}, model={model} with accuracy={accuracy}")
    except Exception as e:
        logger.error(f"Error with feat={feat}, dim={dim}, model={model}: {e}")

    return accuracy

# List of function calls for all combinations
function_calls = []

# Define all possible combinations of feat, dim, and model
feats = [
        "bow",
        # "tfidf",
        #  "d2v",
        #  "w2v"
         ]

dims = [
        "nonee",
        # "pca",
        # "nmf",
        # "lsa"
        ]

models = [
          "mnb",
          # "bnb",
          # "lr",
          # "rf",
          # "svm",
          # "knn",
          # "wknn"
          ]

# Generate the log filename
log_filename = get_log_filename(base_name='model_results')

# Create a logger object
logger = logging.getLogger('experiment_logger')
logger.setLevel(logging.INFO)

# Create a file handler for logging with the dynamic filename
file_handler = logging.FileHandler(log_filename)
file_handler.setLevel(logging.INFO)

# Create a logging format
formatter = logging.Formatter('%(asctime)s - %(message)s')
file_handler.setFormatter(formatter)

# Add the handler to the logger
logger.addHandler(file_handler)

# Generate function calls for all combinations
for feat in feats:
    for dim in dims:
        for model in models:
            function_calls.append((True, feat, dim, model, test_train_neg_clean, test_train_pos_clean, test_test_neg_clean, test_test_pos_clean))

# # Execute each function call
# for call in function_calls:
#     run_cml(*call)

with Pool(processes=4) as pool:  # Adjust number of processes based on your CPU cores
    results = pool.map(run_single_experiment, function_calls)

files.download(log_filename)

print(len(vectorizer.vocabulary_))

run_cml(True,"w2v","nb",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"w2v","lr",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"w2v","rf",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"w2v","svm",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"w2v","knn",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"w2v","wknn",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)

run_cml(True,"tfidf","nb",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"pca","nb",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"nmf","nb",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"lsa","nb",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"d2v","nb",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"tfidf","lr",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"pca","lr",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"nmf","lr",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"lsa","lr",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"d2v","lr",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"tfidf","rf",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"pca","rf",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"nmf","rf",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"lsa","rf",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"d2v","rf",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"tfidf","svm",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"pca","svm",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"nmf","svm",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"lsa","svm",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"d2v","svm",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"tfidf","knn",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"pca","knn",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"nmf","knn",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"lsa","knn",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"d2v","knn",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"tfidf","wknn",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"pca","wknn",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"nmf","wknn",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"lsa","wknn",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_cml(True,"d2v","wknn",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)

def run_dml(once,feat,dim,model,test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean):
  print(once,' ',feat, ' ', dim ,' ',model)
  # print(clean_voc.most_common(50))
  tokens= [k for k,c in clean_voc.items() if c < 3]
  # print(tokens)
  acc=[]
  if once:
    # print("run--50")
    # to_remove=tokens+rem_words(50)
    # # print(" to remove-"+str(len(to_remove))+' '+str(len(to_remove)-7275))
    # test_train_neg_clean=remove_vocab(train_neg_clean,to_remove)
    # test_train_pos_clean=remove_vocab(train_pos_clean,to_remove)
    # test_test_neg_clean=remove_vocab(test_neg_clean,to_remove)
    # test_test_pos_clean=remove_vocab(test_pos_clean,to_remove)
    # print(" cleaned")
    train=test_train_neg_clean + test_train_pos_clean
    test=test_test_neg_clean + test_test_pos_clean
    # train, test= shuffle(test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
    ran=1
  else:
    ran=10
  for n in range(ran):
    if not once:
      print("run--"+str(n*10))
      to_remove=tokens+rem_words(n*10)
      # print(" to remove-"+str(len(to_remove))+' '+str(len(to_remove)-7275))
      test_train_neg_clean=remove_vocab(train_neg_clean,to_remove)
      test_train_pos_clean=remove_vocab(train_pos_clean,to_remove)
      test_test_neg_clean=remove_vocab(test_neg_clean,to_remove)
      test_test_pos_clean=remove_vocab(test_pos_clean,to_remove)
      print(" cleaned")
      train=test_train_neg_clean + test_train_pos_clean
      test=test_test_neg_clean + test_test_pos_clean
      # train, test= shuffle(test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)

    match feat:
      case "tfidf":
        tfidf_vectorizer = TfidfVectorizer()
        X_train = tfidf_vectorizer.fit_transform(train)
        X_test = tfidf_vectorizer.transform(test)
        X_train = X_train.toarray()
        X_test = X_test.toarray()

      case "w2v":
        vocab_size = len(word2vec_model.wv)
        embedding_dim = word2vec_model.vector_size

        train_docs = [filter_words(review) for review in train]
        test_docs = [filter_words(review) for review in test]

        X_train = [[w2v_model.wv[word] for word in review] for review in train_docs]
        # X_train = pad_sequences(embedded_reviews, maxlen=200, padding='post', truncating='post').astype('float32')

        X_test = [[w2v_model.wv[word] for word in review] for review in test_docs]
        # X_test = pad_sequences(embedded_reviews, maxlen=200, padding='post', truncating='post').astype('float32')

        X_train = average_document_vectors(X_train)
        X_test = average_document_vectors(X_test)

        # X_train = document_vectorizer(train, w2v_model, num_features=w2v_model.vector_size)
        # X_test =document_vectorizer(test, w2v_model, num_features=w2v_model.vector_size)

      case "d2v":
        # Prepare tagged documents for training Doc2Vec
        tagged_train_docs = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(train)]
        tagged_test_docs = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(test)]
        X_train = np.array([doc2vec_model.infer_vector(doc.words) for doc in tagged_train_docs])
        X_test = np.array([doc2vec_model.infer_vector(doc.words) for doc in tagged_test_docs])


    match dim:
      case "pca":
        pca = PCA(n_components=100)  # Adjust n_components as needed

        # if feat=="tfidf":
        #   X_train=X_train.toarray()
        #   X_test=X_test.toarray()
        # Fit PCA on TF-IDF transformed training data and transform training/test data
        X_train = pca.fit_transform(X_train)
        X_test = pca.transform(X_test)

      case "nmf":
        nmf = NMF(n_components=10, random_state=42)
        if not feat == "tfidf":
          if np.any(X_train < 0) or np.any(X_test < 0):
            offset = np.abs(np.min(X_train)) + 1
            X_train += offset
            X_test += offset

        X_train = nmf.fit_transform(X_train)
        X_test = nmf.transform(X_test)

      case "lsa":
        lsa = TruncatedSVD(n_components=100)

        X_train = lsa.fit_transform(X_train)
        X_test = lsa.transform(X_test)

      case "nonee":
        print('none')

      # case _:
      #   print("nyumere1")
      #   break
    # X_train_tfidf = tfidf_vectorizer.fit_transform(train)
    # X_test_tfidf = tfidf_vectorizer.transform(test)
    # print(X_train_tfidf[1],'-------------------------')
    y_train = array([0 for _ in range(6000)] + [1 for _ in range(6000)])
    y_test = array([0 for _ in range(1500)] + [1 for _ in range(1500)])

    match model:
      case "dnn":
        input_dim = X_train.shape[1]

        model = Sequential([
            Dense(128, activation='relu', input_dim=input_dim),
            Dropout(0.5),
            Dense(64, activation='relu'),
            Dropout(0.5),
            Dense(1, activation='sigmoid')
          ])
        # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

        # history = model.fit(X_train, y_train, epochs=1, batch_size=32,validation_data=(X_test, y_test))

      case "cnn":
        # input_dim = X_train.shape[1]

        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  # (samples, features, 1)
        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)       # (samples, features, 1)

        model = Sequential([
            Conv1D(filters=128, kernel_size=5, activation='relu',input_shape=(X_train.shape[1], 1)),
            MaxPooling1D(pool_size=2),
            Flatten(),
            Dense(1, activation='sigmoid')
          ])
        # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

        # history = model.fit(X_train, y_train, epochs=1, batch_size=32,validation_data=(X_test, y_test))

      case "blstm":
        input_dim = X_train.shape[1]

        timestep_size = 1  # Define the number of timesteps
        X_train = X_train.reshape(X_train.shape[0], timestep_size, X_train.shape[1])
        X_test = X_test.reshape(X_test.shape[0], timestep_size, X_test.shape[1])

        model = keras.Sequential([
          keras.layers.Bidirectional(keras.layers.LSTM(64, activation='tanh', recurrent_dropout=0, input_shape=(timestep_size, X_train.shape[1]))),
          keras.layers.Dense(24, activation='relu'),
          keras.layers.Dense(1, activation='sigmoid')
        ])

        # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

        # history = model.fit(X_train, y_train, epochs=1, batch_size=32,validation_data=(X_test, y_test))

      case "lstm":
        input_dim = X_train.shape[1]

        timestep_size = 1  # Define the number of timesteps
        X_train = X_train.reshape(X_train.shape[0], timestep_size, X_train.shape[1])
        X_test = X_test.reshape(X_test.shape[0], timestep_size, X_test.shape[1])

        model = keras.Sequential([
          keras.layers.LSTM(64, activation='tanh', recurrent_dropout=0, input_shape=(timestep_size, X_train.shape[2])),
          keras.layers.Dense(24, activation='relu'),
          keras.layers.Dense(1, activation='sigmoid')
        ])


    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    history = model.fit(X_train, y_train, epochs=1, batch_size=32,validation_data=(X_test, y_test))

    # Evaluate the model
    _, accuracy = model.evaluate(X_test, y_test)
    print(f"Test Accuracy: {accuracy}")

  return acc

# List of function calls for all combinations
function_calls = []

# Define all possible combinations of feat, dim, and model
feats = [
        "tfidf",
         "d2v",
         "w2v"
         ]

dims = [
        "nonee",
        "pca",
        "nmf",
        "lsa"
        ]

models = [
          "cnn",
          "lstm",
          "blstm",
          "dnn"
          ]

# Generate a unique log file name based on the counter
log_counter += 1
filename='model_results_'+str( log_counter)+'.log'
logging.basicConfig(filename, level=logging.INFO)

# Generate function calls for all combinations
for feat in feats:
    for dim in dims:
        for model in models:
            function_calls.append((True, feat, dim, model, test_train_neg_clean, test_train_pos_clean, test_test_neg_clean, test_test_pos_clean))

# # Execute each function call
# for call in function_calls:
#     logging.info(f"Running with settings: feat={call[1]}, dim={call[2]}, model={call[3]}")
#     accuracy = run_cml(*call)
#     logging.info(f"Resulting accuracy: {accuracy}")



run_dml(True,"tfidf",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_dml(True,"pca",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_dml(True,"nmf",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_dml(True,"lsa",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)
run_dml(True,"d2v",test_train_neg_clean , test_train_pos_clean , test_test_neg_clean , test_test_pos_clean)

# |                          | Naive Bayes | Logistic Regression | Random Forest | SVM | k-NN | Weighted k-NN |
# |--------------------------|-------------|---------------------|---------------|-----|------|---------------|
# | **TF-IDF**               |             |                     |               |     |      |               |
# | None                     | 92%         | 93%                 | 92%           | 93% | 82%  | 82%           |
# | PCA                      | 85%         | 91%                 | 88%           | 92% | 83%  | 83%           |
# | NMF                      | 85%         | 85%                 | 89%           | 86% | 85%  | 85%           |
# | LSA                      | 85%         | 92%                 | 88%           | 92% | 83%  | 84%           |
# | **Doc2Vec**              |             |                     |               |     |      |               |
# | None                     | 83%         | 86%                 | 83%           | 86% | 80%  | 80%           |
# | PCA                      | 83%         | 86%                 | 84%           | 86% | 81%  | 80%           |
# | NMF                      | 56%         | 50%                 | 49%           | 50% | 50%  | 50%           |
# | LSA                      | 84%         | 87%                 | 83%           | 87% | 80%  | 80%           |
# | **Word2Vec**             |             |                     |               |     |      |               |
# | None                     | 74%         | 85%                 | 85%           | 87% | 81%  | 81%           |
# | PCA                      | 74%         | 85%                 | 83%           | 87% | 81%  | 81%           |
# | NMF                      | 50%         | 51%                 | 48%           | 59% | 50%  | 50%           |
# | LSA                      | 74%         | 85%                 | 83%           | 87% | 81%  | 81%           |
#
# |                     | CNN    | LSTM   | BiLSTM | DNN    |
# |---------------------|--------|--------|--------|--------|
# | **TF-IDF None-E**   | 92.33% | 93.00% | 92.80% | 93.00% |
# | **TF-IDF PCA**      | 83.50% | 91.13% | 91.37% | 90.63% |
# | **TF-IDF NMF**      | 75.63% | 86.00% | 85.43% | 86.40% |
# | **TF-IDF LSA**      | 87.37% | 91.30% | 91.30% | 91.00% |
# | **Doc2Vec None-E**  | 83.67% | 86.13% | 86.40% | 86.17% |
# | **Doc2Vec PCA**     | 84.33% | 86.53% | 86.47% | 86.17% |
# | **Doc2Vec NMF**     | 75.63% | 86.00% | 85.43% | 86.40% |
# | **Doc2Vec LSA**     | 83.10% | 85.87% | 86.73% | 86.87% |
# | **Word2Vec None-E** | 83.37% | 85.17% | 84.83% | 84.23% |
# | **Word2Vec PCA**    | 74.77% | 85.93% | 85.90% | 86.43% |
# | **Word2Vec NMF**    | 50.03% | 50.00% | 50.00% | 50.00% |
# | **Word2Vec LSA**    | 81.43% | 85.53% | 85.70% | 85.57% |
